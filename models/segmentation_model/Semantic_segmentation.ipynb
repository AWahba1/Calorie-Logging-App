{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97803c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-19T22:51:08.645895Z",
     "iopub.status.busy": "2023-05-19T22:51:08.645159Z",
     "iopub.status.idle": "2023-05-19T22:51:13.695155Z",
     "shell.execute_reply": "2023-05-19T22:51:13.694583Z",
     "shell.execute_reply.started": "2023-05-19T22:51:08.645851Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444760a-ee67-4ead-bbd5-652950b913f3",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7577a2-a3a6-40b2-a67b-e9d132cd67ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:40:28.660803Z",
     "iopub.status.busy": "2023-05-14T13:40:28.659811Z",
     "iopub.status.idle": "2023-05-14T13:40:28.666782Z",
     "shell.execute_reply": "2023-05-14T13:40:28.665746Z",
     "shell.execute_reply.started": "2023-05-14T13:40:28.660758Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def resize(input_image, input_mask):\n",
    "   input_image = tf.image.resize(input_image, (128, 128), method=\"nearest\")\n",
    "   input_mask = tf.image.resize(input_mask, (128, 128), method=\"nearest\")\n",
    "   return input_image, input_mask\n",
    "\n",
    "def augment(input_image, input_mask):\n",
    "   if tf.random.uniform(()) > 0.5:\n",
    "       # Random flipping of the image and mask\n",
    "       input_image = tf.image.flip_left_right(input_image)\n",
    "       input_mask = tf.image.flip_left_right(input_mask)\n",
    "   return input_image, input_mask\n",
    "\n",
    "\n",
    "def normalize(input_image, input_mask):\n",
    "   input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "   input_mask -= 1\n",
    "   return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54506dc7-af7e-4bc5-8c36-58d38928df55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T16:52:34.096357Z",
     "iopub.status.busy": "2023-05-18T16:52:34.095769Z",
     "iopub.status.idle": "2023-05-18T16:54:47.115492Z",
     "shell.execute_reply": "2023-05-18T16:54:47.114780Z",
     "shell.execute_reply.started": "2023-05-18T16:52:34.096326Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://mm.cs.uec.ac.jp/uecfoodpix/UECFOODPIXCOMPLETE.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08377b58-0daa-47dd-9b9e-5efe7ea71830",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-14T13:54:34.679148Z",
     "iopub.status.busy": "2023-05-14T13:54:34.678815Z",
     "iopub.status.idle": "2023-05-14T13:54:36.242453Z",
     "shell.execute_reply": "2023-05-14T13:54:36.241350Z",
     "shell.execute_reply.started": "2023-05-14T13:54:34.679120Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/food201/food201.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640108b-ad49-4583-8c98-8dad132cfb44",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-14T13:58:01.725910Z",
     "iopub.status.busy": "2023-05-14T13:58:01.724873Z",
     "iopub.status.idle": "2023-05-14T13:58:12.765932Z",
     "shell.execute_reply": "2023-05-14T13:58:12.764703Z",
     "shell.execute_reply.started": "2023-05-14T13:58:01.725872Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!unzip food201.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f121d7a2-7d28-4263-865b-f9a9a978e95f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-15T17:48:42.908241Z",
     "iopub.status.busy": "2023-05-15T17:48:42.907839Z",
     "iopub.status.idle": "2023-05-15T17:49:36.965069Z",
     "shell.execute_reply": "2023-05-15T17:49:36.963071Z",
     "shell.execute_reply.started": "2023-05-15T17:48:42.908194Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "src_dir = './images'\n",
    "\n",
    "output_dir = './seg-dataset'\n",
    "\n",
    "food201_masks_dir='./food201/pixel_annotations'\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "\n",
    "\n",
    "for class_name in os.listdir(src_dir):\n",
    "    class_dir = os.path.join(src_dir, class_name)\n",
    "\n",
    "    image_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "\n",
    "    train_files, test_files = train_test_split(image_files, test_size=test_size)\n",
    "    \n",
    "    # print(train_files)\n",
    "    \n",
    "    \n",
    "    # Create 4 directories \n",
    "    train_image_dir = os.path.join(output_dir, 'train-image', class_name)\n",
    "    test_image_dir = os.path.join(output_dir, 'test-image', class_name)\n",
    "    train_mask_dir=os.path.join(output_dir, 'train-mask', class_name)\n",
    "    test_mask_dir=os.path.join(output_dir, 'test-mask', class_name)\n",
    "    \n",
    "    os.makedirs(train_image_dir, exist_ok=True)\n",
    "    os.makedirs(test_image_dir, exist_ok=True)\n",
    "    os.makedirs(train_mask_dir, exist_ok=True)\n",
    "    os.makedirs(test_mask_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # for f in train_files:\n",
    "    #     shutil.copy(f, os.path.join(train_class_dir, os.path.basename(f)))\n",
    "\n",
    "#     for f in test_files:\n",
    "#         shutil.copy(f, os.path.join(test_class_dir, os.path.basename(f)))\n",
    "        \n",
    "    \n",
    "    for f in train_files:\n",
    "        file_name = os.path.basename(f)[:-4]  # Remove the file extension\n",
    "        mask_file = os.path.join(food201_masks_dir, class_name,file_name + '.png')\n",
    "\n",
    "        # Copy the image file to the train directory\n",
    "        shutil.copy(f, os.path.join(train_image_dir, os.path.basename(f)))\n",
    "\n",
    "        # Copy the mask file to the train mask directory\n",
    "        shutil.copy(mask_file, os.path.join(train_mask_dir, os.path.splitext(os.path.basename(f))[0] + '.png'))\n",
    "\n",
    "    for f in test_files:\n",
    "        file_name = os.path.basename(f)[:-4]  # Remove the file extension\n",
    "        mask_file = os.path.join(food201_masks_dir, class_name,file_name + '.png')\n",
    "\n",
    "        # Copy the image file to the train directory\n",
    "        shutil.copy(f, os.path.join(test_image_dir, os.path.basename(f)))\n",
    "\n",
    "        # Copy the mask file to the train mask directory\n",
    "        shutil.copy(mask_file, os.path.join(test_mask_dir, os.path.splitext(os.path.basename(f))[0] + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0df0e352-a2b7-4dc8-8798-e980c3deb467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T18:43:25.637036Z",
     "iopub.status.busy": "2023-05-18T18:43:25.636542Z",
     "iopub.status.idle": "2023-05-18T18:43:31.681339Z",
     "shell.execute_reply": "2023-05-18T18:43:31.680695Z",
     "shell.execute_reply.started": "2023-05-18T18:43:25.637016Z"
    }
   },
   "outputs": [],
   "source": [
    "# MERGE UECFOODPIXCOMPLETE with Food201\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "output_dir = './seg-dataset'\n",
    "\n",
    "uec_train_image_dir='./UECFOODPIXCOMPLETE/data/UECFoodPIXCOMPLETE/train/img'\n",
    "uec_train_mask_dir='./UECFOODPIXCOMPLETE/data/UECFoodPIXCOMPLETE/train/mask'\n",
    "\n",
    "uec_test_image_dir='./UECFOODPIXCOMPLETE/data/UECFoodPIXCOMPLETE/test/img'\n",
    "uec_test_mask_dir='./UECFOODPIXCOMPLETE/data/UECFoodPIXCOMPLETE/test/mask'\n",
    "\n",
    "uec_test_dir='./UECFoodPIXCOMPLETE/test/img'\n",
    "class_name=\"uec\"\n",
    "\n",
    "train_image_dir = os.path.join(output_dir, 'train-image', class_name)\n",
    "test_image_dir = os.path.join(output_dir, 'test-image', class_name)\n",
    "train_mask_dir=os.path.join(output_dir, 'train-mask', class_name)\n",
    "test_mask_dir=os.path.join(output_dir, 'test-mask', class_name)\n",
    "\n",
    "os.makedirs(train_image_dir, exist_ok=True)\n",
    "os.makedirs(test_image_dir, exist_ok=True)\n",
    "os.makedirs(train_mask_dir, exist_ok=True)\n",
    "os.makedirs(test_mask_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for f in os.listdir(uec_test_image_dir):\n",
    "        file_name = os.path.basename(f)[:-4]  # Remove the file extension\n",
    "        image_file=mask_file = os.path.join(uec_test_image_dir,file_name + '.jpg')\n",
    "        mask_file = os.path.join(uec_test_mask_dir,file_name + '.png')\n",
    "\n",
    "        # Copy the image file to the train directory\n",
    "        shutil.copy(image_file, os.path.join(test_image_dir, os.path.basename(f)))\n",
    "        \n",
    "\n",
    "        # Copy the mask file to the train mask directory\n",
    "        shutil.copy(mask_file, os.path.join(test_mask_dir, os.path.splitext(os.path.basename(f))[0] + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "06e9a380-09ed-44e5-a529-69657f1edee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T17:52:36.177071Z",
     "iopub.status.busy": "2023-05-15T17:52:36.176520Z",
     "iopub.status.idle": "2023-05-15T17:52:36.481788Z",
     "shell.execute_reply": "2023-05-15T17:52:36.473403Z",
     "shell.execute_reply.started": "2023-05-15T17:52:36.177028Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10839\n",
      "10839\n",
      "1254\n",
      "1254\n",
      "12093\n",
      "----------\n",
      "10839\n",
      "1254\n"
     ]
    }
   ],
   "source": [
    "# Check all images exist after splitting\n",
    "\n",
    "train_image_dir = './seg-dataset/train-image'\n",
    "train_mask_dir = './seg-dataset/train-mask'\n",
    "\n",
    "test_image_dir = './seg-dataset/test-image'\n",
    "test_mask_dir = './seg-dataset/test-mask'\n",
    "\n",
    "food201_masks_dir='./food201/pixel_annotations'\n",
    "\n",
    "def get_image_filenames(image_dir):\n",
    "        image_filenames = []\n",
    "        for class_dir in os.listdir(image_dir):\n",
    "            class_image_dir = os.path.join(image_dir, class_dir)\n",
    "            class_image_filenames = os.listdir(class_image_dir)\n",
    "            for image_filename in class_image_filenames:\n",
    "                image_filenames.append(os.path.join(class_dir, image_filename))\n",
    "        return len(image_filenames)\n",
    "    \n",
    "    \n",
    "train_image=get_image_filenames(train_image_dir)\n",
    "train_mask=get_image_filenames(train_mask_dir)\n",
    "test_image=get_image_filenames(test_image_dir)\n",
    "test_mask=get_image_filenames(test_mask_dir)\n",
    "\n",
    "food201_masks=get_image_filenames(food201_masks_dir)\n",
    "\n",
    "# Check every train image has an equuiavent train mask and same for test images\n",
    "\n",
    "def check_missing_images(image_dir, mask_dir):\n",
    "        image_filenames = []\n",
    "        for class_dir in os.listdir(image_dir):\n",
    "            class_image_dir = os.path.join(image_dir, class_dir)\n",
    "            class_image_filenames = os.listdir(class_image_dir)\n",
    "            for image_filename in class_image_filenames:\n",
    "                image_path = os.path.join(class_image_dir, image_filename)\n",
    "                mask_filename = image_filename.split('.')[0] + '.png'\n",
    "                mask_path = os.path.join(mask_dir, class_dir, mask_filename)\n",
    "                if os.path.isfile(mask_path):\n",
    "                    image_filenames.append(os.path.join(class_dir, image_filename))\n",
    "                else:\n",
    "                    print(f\"Warning: No mask found for image '{image_filename}'\")\n",
    "        return len(image_filenames)\n",
    "\n",
    "print(check_missing_images(train_image_dir, train_mask_dir))\n",
    "print(check_missing_images(test_image_dir, test_mask_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2482fe05-c78f-48fa-a6f1-66fb0c760f82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-19T22:51:19.998549Z",
     "iopub.status.busy": "2023-05-19T22:51:19.998097Z",
     "iopub.status.idle": "2023-05-19T22:51:20.955351Z",
     "shell.execute_reply": "2023-05-19T22:51:20.954639Z",
     "shell.execute_reply.started": "2023-05-19T22:51:19.998524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19839\n",
      "2254\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_dir, mask_dir, batch_size, shuffle=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.classes = os.listdir(image_dir)\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.image_filenames = self.get_image_filenames()\n",
    "        self.num_samples = len(self.image_filenames)\n",
    "        self.image_size=(224,224)\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.image_filenames))\n",
    "        self.food_item_ids = self.get_food_item_ids()\n",
    "        # self.transform=self.get_transform()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    \n",
    "    def get_food_item_ids(self):\n",
    "        csv_file = './food201/pixel_annotations_map.csv'  \n",
    "        food_ids = []\n",
    "\n",
    "        with open(csv_file, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                if row:  # Skip empty rows\n",
    "                    id = int(row[0].strip())\n",
    "                    food_ids.append(id)\n",
    "        return food_ids\n",
    "        \n",
    "        \n",
    "    def get_image_filenames(self):\n",
    "        image_filenames = []\n",
    "        for class_dir in self.classes:\n",
    "            # if class_dir !=\"uec\":\n",
    "            #     continue\n",
    "            class_image_dir = os.path.join(self.image_dir, class_dir)\n",
    "            class_image_filenames = os.listdir(class_image_dir)\n",
    "            for image_filename in class_image_filenames:\n",
    "                image_filenames.append(os.path.join(class_dir, image_filename))\n",
    "            # print(image_filenames)\n",
    "        return image_filenames\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.num_samples / float(self.batch_size)))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.shuffle:\n",
    "            batch_image_filenames = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        else:\n",
    "            batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            batch_image_filenames = [self.image_filenames[i] for i in batch_indexes]\n",
    "        batch_images = []\n",
    "        batch_masks = []\n",
    "        \n",
    "        for image_filename in batch_image_filenames:\n",
    "            # Paths\n",
    "            image_path = os.path.join(self.image_dir, image_filename)\n",
    "            \n",
    "            mask_filename = image_filename.split('.')[0] + '.png'\n",
    "            mask_path = os.path.join(self.mask_dir, mask_filename)\n",
    "            \n",
    "            is_uec = image_filename.split(\"/\")[0] == \"uec\"\n",
    "\n",
    "            # Preprocess Image\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, self.image_size)\n",
    "            \n",
    "            # Preprocess Mask according to which dataset it belongs to\n",
    "            mask=None\n",
    "            \n",
    "            if is_uec:\n",
    "                mask = cv2.imread(mask_path)\n",
    "                mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "                mask = cv2.resize(mask, self.image_size)\n",
    "                mask = np.where(mask != 0, 1, 0)\n",
    "                \n",
    "            else: # Food201 dataset\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                mask = cv2.resize(mask, self.image_size)\n",
    "                mask = np.where(np.isin(mask, self.food_item_ids), 1, 0) # Threshold the mask to convert it to binar\n",
    "\n",
    "             # Normalize the image\n",
    "            image = image / 255.0\n",
    "            \n",
    "            batch_images.append(image)\n",
    "            batch_masks.append(mask)\n",
    "        \n",
    "        return np.array(batch_images), np.array(batch_masks)\n",
    "    \n",
    "\n",
    "train_image_dir = './seg-dataset/train-image'\n",
    "train_mask_dir = './seg-dataset/train-mask'\n",
    "\n",
    "test_image_dir = './seg-dataset/test-image'\n",
    "test_mask_dir = './seg-dataset/test-mask'\n",
    "\n",
    "batch_size=32\n",
    "img_height=224\n",
    "img_width=224\n",
    "\n",
    "train_generator = DataGenerator(train_image_dir, train_mask_dir, batch_size)\n",
    "val_generator = DataGenerator(test_image_dir, test_mask_dir, batch_size, shuffle=False)\n",
    "\n",
    "num_training_samples=train_generator.num_samples\n",
    "num_validation_samples = val_generator.num_samples\n",
    "\n",
    "print(num_training_samples)\n",
    "print(num_validation_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289c9084-a423-402b-94ce-9d13eb6a82d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T11:27:29.755696Z",
     "iopub.status.busy": "2023-05-15T11:27:29.754586Z",
     "iopub.status.idle": "2023-05-15T11:27:31.533938Z",
     "shell.execute_reply": "2023-05-15T11:27:31.533200Z",
     "shell.execute_reply.started": "2023-05-15T11:27:29.755667Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10839 images belonging to 101 classes.\n",
      "Found 10839 images belonging to 101 classes.\n",
      "Found 1254 images belonging to 101 classes.\n",
      "Found 1254 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the directories for the training data and the masks\n",
    "train_orig_dir = './seg-dataset/train-original'\n",
    "train_mask_dir = './seg-dataset/train-mask'\n",
    "\n",
    "batch_size=16\n",
    "img_height=224\n",
    "img_width=224\n",
    "\n",
    "def threshold_mask(mask):\n",
    "    mask[mask > 0] = 1\n",
    "    mask[mask <= 0] = 0\n",
    "    return mask\n",
    "\n",
    "# Create an ImageDataGenerator for the original images\n",
    "data_gen_args = dict(rescale=1./255)\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "image_generator = image_datagen.flow_from_directory(train_orig_dir,\n",
    "                                                     class_mode=None,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     target_size=(img_height, img_width))\n",
    "\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args, preprocessing_function=threshold_mask)\n",
    "mask_generator = mask_datagen.flow_from_directory(train_mask_dir,\n",
    "                                                   class_mode=None,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   target_size=(img_height, img_width),\n",
    "                                                   color_mode='grayscale')\n",
    "\n",
    "\n",
    "num_training_samples=len(mask_generator)\n",
    "\n",
    "# Combine the generators into one that yields image and mask pairs\n",
    "train_generator = zip(image_generator, mask_generator)\n",
    "\n",
    "# num_training_samples=len(train_generator)\n",
    "\n",
    "# print(f\"Num of training samples {num_training_samples}\")\n",
    "\n",
    "\n",
    "# Set the directories for the test data and the masks\n",
    "test_orig_dir = './seg-dataset/test-original'\n",
    "test_mask_dir = './seg-dataset/test-mask'\n",
    "\n",
    "# Create an ImageDataGenerator for the original test images\n",
    "test_image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "test_image_generator = test_image_datagen.flow_from_directory(test_orig_dir,\n",
    "                                                               class_mode=None,\n",
    "                                                               batch_size=batch_size,\n",
    "                                                               target_size=(img_height, img_width))\n",
    "\n",
    "# Create an ImageDataGenerator for the test masks\n",
    "test_mask_datagen = ImageDataGenerator(**data_gen_args, preprocessing_function=threshold_mask)\n",
    "test_mask_generator = test_mask_datagen.flow_from_directory(test_mask_dir,\n",
    "                                                             class_mode=None,\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             target_size=(img_height, img_width),\n",
    "                                                             color_mode='grayscale')\n",
    "\n",
    "num_validation_samples=len(test_mask_generator)\n",
    "\n",
    "# Combine the generators into one that yields test image and mask pairs\n",
    "test_generator = zip(test_image_generator, test_mask_generator)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792556a-b2cc-4afa-93d5-271c62105588",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2b5738-745c-47e6-98e9-9ee071c5f402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-19T22:52:34.001267Z",
     "iopub.status.busy": "2023-05-19T22:52:34.001014Z",
     "iopub.status.idle": "2023-05-19T22:52:34.007816Z",
     "shell.execute_reply": "2023-05-19T22:52:34.006697Z",
     "shell.execute_reply.started": "2023-05-19T22:52:34.001249Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def double_conv_block(x, n_filters):\n",
    "   # Conv2D then ReLU activation\n",
    "   x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n",
    "   x = layers.BatchNormalization()(x)\n",
    "   # x = layers.Activation(\"relu\")(x)\n",
    "   x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "\n",
    "   # Conv2D then ReLU activation\n",
    "   x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n",
    "   x = layers.BatchNormalization()(x)\n",
    "   x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "   x = layers.Dropout(0.2)(x)\n",
    "   # x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "   return x\n",
    "\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "   f = double_conv_block(x, n_filters)\n",
    "   p = layers.MaxPool2D(2)(f)\n",
    "   # p = layers.Dropout(0.3)(p)\n",
    "   return f, p\n",
    "\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "   # upsample\n",
    "   x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "   # concatenate\n",
    "   x = layers.concatenate([x, conv_features])\n",
    "   # dropout\n",
    "   # x = layers.Dropout(0.3)(x)\n",
    "   # Conv2D twice with ReLU activation\n",
    "   x = double_conv_block(x, n_filters)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51e4b20-53c6-49f6-9184-a86f996011a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-19T22:52:37.251492Z",
     "iopub.status.busy": "2023-05-19T22:52:37.250846Z",
     "iopub.status.idle": "2023-05-19T22:52:37.256512Z",
     "shell.execute_reply": "2023-05-19T22:52:37.255745Z",
     "shell.execute_reply.started": "2023-05-19T22:52:37.251467Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_unet_model():\n",
    "     # inputs\n",
    "   inputs = layers.Input(shape=(224,224,3))\n",
    "\n",
    "   # encoder: contracting path - downsample\n",
    "   # 1 - downsample\n",
    "   f1, p1 = downsample_block(inputs, 64)\n",
    "   # 2 - downsample\n",
    "   f2, p2 = downsample_block(p1, 128)\n",
    "   # 3 - downsample\n",
    "   f3, p3 = downsample_block(p2, 256)\n",
    "   # 4 - downsample\n",
    "   f4, p4 = downsample_block(p3, 512)\n",
    "\n",
    "   # 5 - bottleneck\n",
    "   bottleneck = double_conv_block(p4, 1024)\n",
    "\n",
    "   # decoder: expanding path - upsample\n",
    "   # 6 - upsample\n",
    "   u6 = upsample_block(bottleneck, f4, 512)\n",
    "   # 7 - upsample\n",
    "   u7 = upsample_block(u6, f3, 256)\n",
    "   # 8 - upsample\n",
    "   u8 = upsample_block(u7, f2, 128)\n",
    "   # 9 - upsample\n",
    "   u9 = upsample_block(u8, f1, 64)\n",
    "\n",
    "   # outputs\n",
    "   outputs = layers.Conv2D(1, 1, padding=\"same\", activation = \"sigmoid\")(u9)\n",
    "\n",
    "   # unet model with Keras Functional API\n",
    "   unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "\n",
    "   return unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd5a45-e7bc-4d8e-8e07-8a258bdd16b6",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81626b2-20f4-4557-83c3-017eef4e0a4a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b865741-c9c7-4dac-bafe-83d54d49b6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-19T22:52:39.235321Z",
     "iopub.status.busy": "2023-05-19T22:52:39.234718Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "model=build_unet_model()\n",
    "\n",
    "# model=build_unet_model_pretrained()\n",
    "# tf.keras.utils.plot_model(model, \"model.png\", show_shapes=False, show_dtype=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)\n",
    "\n",
    "model.compile(optimizer=Adam(1e-3),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[ tf.keras.metrics.BinaryAccuracy() , tf.keras.metrics.BinaryIoU()]\n",
    "              # tf.keras.metrics.Recall(name=\"recall\"),\n",
    "              # tf.keras.metrics.Precision(name=\"precision\"),\n",
    "              )\n",
    "\n",
    "\n",
    "csv_logger=CSVLogger('segmentation-unet.log')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath='./U-Net/model_weights.{epoch:02d}.h5', \n",
    "                                       save_weights_only=True)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    \n",
    "    patience=10,            \n",
    "    restore_best_weights=True   \n",
    ")\n",
    "\n",
    "epochs=100\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5)\n",
    "\n",
    "model_history = model.fit(train_generator,\n",
    "                    validation_steps=num_validation_samples // batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    steps_per_epoch = num_training_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early_stopping, csv_logger, checkpoint_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0994aa-b45f-4385-9e50-7174f910283a",
   "metadata": {},
   "source": [
    "## Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23fb9c64-61f6-4c60-a817-84b92fdb716f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-19T11:21:09.388079Z",
     "iopub.status.busy": "2023-05-19T11:21:09.387319Z",
     "iopub.status.idle": "2023-05-19T12:43:04.404364Z",
     "shell.execute_reply": "2023-05-19T12:43:04.403515Z",
     "shell.execute_reply.started": "2023-05-19T11:21:09.388059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 11:21:24.607141: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-19 11:21:24.607231: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-19 11:21:25.100905: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-19 11:21:25.100979: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-19 11:26:15.776173: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.01GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-05-19 11:26:15.776254: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.01GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 - 435s - loss: 0.1825 - binary_accuracy: 0.9286 - binary_io_u: 0.8652 - val_loss: 0.2591 - val_binary_accuracy: 0.9067 - val_binary_io_u: 0.8280 - 435s/epoch - 703ms/step\n",
      "Epoch 22/100\n",
      "619/619 - 401s - loss: 0.1751 - binary_accuracy: 0.9311 - binary_io_u: 0.8695 - val_loss: 0.2543 - val_binary_accuracy: 0.9033 - val_binary_io_u: 0.8213 - 401s/epoch - 648ms/step\n",
      "Epoch 23/100\n",
      "619/619 - 407s - loss: 0.1640 - binary_accuracy: 0.9360 - binary_io_u: 0.8782 - val_loss: 0.2570 - val_binary_accuracy: 0.9064 - val_binary_io_u: 0.8273 - 407s/epoch - 657ms/step\n",
      "Epoch 24/100\n",
      "619/619 - 401s - loss: 0.1559 - binary_accuracy: 0.9392 - binary_io_u: 0.8838 - val_loss: 0.2749 - val_binary_accuracy: 0.9060 - val_binary_io_u: 0.8264 - 401s/epoch - 648ms/step\n",
      "Epoch 25/100\n",
      "619/619 - 401s - loss: 0.1459 - binary_accuracy: 0.9438 - binary_io_u: 0.8920 - val_loss: 0.2856 - val_binary_accuracy: 0.8996 - val_binary_io_u: 0.8149 - 401s/epoch - 648ms/step\n",
      "Epoch 26/100\n",
      "619/619 - 401s - loss: 0.1371 - binary_accuracy: 0.9476 - binary_io_u: 0.8990 - val_loss: 0.2886 - val_binary_accuracy: 0.9037 - val_binary_io_u: 0.8220 - 401s/epoch - 648ms/step\n",
      "Epoch 27/100\n",
      "619/619 - 402s - loss: 0.1306 - binary_accuracy: 0.9502 - binary_io_u: 0.9038 - val_loss: 0.2965 - val_binary_accuracy: 0.9030 - val_binary_io_u: 0.8216 - 402s/epoch - 649ms/step\n",
      "Epoch 28/100\n",
      "619/619 - 407s - loss: 0.1248 - binary_accuracy: 0.9526 - binary_io_u: 0.9081 - val_loss: 0.3286 - val_binary_accuracy: 0.9038 - val_binary_io_u: 0.8231 - 407s/epoch - 658ms/step\n",
      "Epoch 29/100\n",
      "619/619 - 411s - loss: 0.1194 - binary_accuracy: 0.9548 - binary_io_u: 0.9122 - val_loss: 0.3012 - val_binary_accuracy: 0.8986 - val_binary_io_u: 0.8134 - 411s/epoch - 664ms/step\n",
      "Epoch 30/100\n",
      "619/619 - 402s - loss: 0.1142 - binary_accuracy: 0.9569 - binary_io_u: 0.9160 - val_loss: 0.3174 - val_binary_accuracy: 0.9074 - val_binary_io_u: 0.8292 - 402s/epoch - 649ms/step\n",
      "Epoch 31/100\n",
      "619/619 - 401s - loss: 0.1099 - binary_accuracy: 0.9586 - binary_io_u: 0.9192 - val_loss: 0.3131 - val_binary_accuracy: 0.9022 - val_binary_io_u: 0.8195 - 401s/epoch - 648ms/step\n",
      "Epoch 32/100\n",
      "619/619 - 401s - loss: 0.1070 - binary_accuracy: 0.9597 - binary_io_u: 0.9214 - val_loss: 0.3616 - val_binary_accuracy: 0.8742 - val_binary_io_u: 0.7725 - 401s/epoch - 648ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "\n",
    "model=build_unet_model()\n",
    "model.load_weights('/notebooks/U-Net/model_weights.20.h5')\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(1e-3),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[ tf.keras.metrics.BinaryAccuracy() , tf.keras.metrics.BinaryIoU()]\n",
    "              )\n",
    "\n",
    "\n",
    "csv_logger=CSVLogger('segmentation-unet-continue.log')\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath='./U-Net/model_weights.{epoch:02d}.h5', \n",
    "                                       save_weights_only=True)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    \n",
    "    patience=10,            \n",
    "    restore_best_weights=True   \n",
    ")\n",
    "\n",
    "epochs=100\n",
    "\n",
    "model_history = model.fit(train_generator,\n",
    "                    validation_steps=num_validation_samples // batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    steps_per_epoch = num_training_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    initial_epoch=20,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early_stopping, csv_logger, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed650916-7912-4e7a-8d14-bc3e8d5931a2",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c155b-e47f-4b95-8946-f39582b44b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T13:43:56.681469Z",
     "iopub.status.busy": "2023-05-17T13:43:56.680993Z",
     "iopub.status.idle": "2023-05-17T13:43:57.451579Z",
     "shell.execute_reply": "2023-05-17T13:43:57.450508Z",
     "shell.execute_reply.started": "2023-05-17T13:43:56.681431Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# model.save('resnet50V2_model.h5')\n",
    "# model = load_model('resnet50V2_model.h5')\n",
    "\n",
    "\n",
    "model.load_weights('./model_weights.20.h5')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img_height=224\n",
    "img_width=224\n",
    "\n",
    "# Load the input image\n",
    "img_path = '../test_images/sin3.jpeg'\n",
    "img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "\n",
    "# Convert the image to a numpy array and normalize its pixel values\n",
    "x = image.img_to_array(img)\n",
    "x /= 255.\n",
    "\n",
    "# Add a batch dimension to the input\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Use the trained model to predict the mask\n",
    "predicted_mask = model.predict(x)[0]\n",
    "\n",
    "threshold = 0.5  # Adjust this threshold as needed\n",
    "predicted_mask = (predicted_mask >= threshold).astype(np.uint8)\n",
    "\n",
    "# # Plot the input image and the predicted mask side-by-side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(predicted_mask, cmap='gray')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
